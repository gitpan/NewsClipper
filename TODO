The purpose of this document is twofold: to give you an idea of what I'd like
to do in future versions of News Clipper, and to give you an idea of what you
can help me implement. :) The more help I get, the faster these ideas will be
implemented. Also, if you have any thoughts on these, please let me know.

- Fix open source web page to have subscribe instructions.
- fix nc os subscribe html
- Do a website that shows how NC can merge multiple information sources like a
  news clipping service.
- Figure out how Yahoo folks can use it.
- Fix @perl in Makefile.PL to point to perlbin instead of relying on the path.
- Write a font stripper handler for Dave K.
- Add a -u switch, which only checks for updates to handlers, but doesn't
  actually create an output file.
- Automate the handler submission process, with cookies to store info about
  the author. Let them log in and update a handler, which gets sent to Coppit
  for final approval. Also have it do some brain-dead checks (default handlers
  actually work, etc.)
- Add a language translation filter.
- Add support for integrating multiple input streams, maybe like this:
  <input name=in1>
  <filter name=filt1>
  <input name=in2>
  <filter name=filt2>
  <merge>
  <filter name=filt3>
  <output name=array>
  or like this:
  <input name=in1>

  <datastream name="one">
    <filter name=selectkeys keys="a_key">
    <filter name=filt1>
  </datastream>

  <datastream name="two">
    <filter name=selectkeys keys="another_key">
    <filter name=filt1>
  </datastream>

  <merge datastreams="one,two">
  <output name="out1">
- Update the Slashdot handler to output number of comments and alt text from
  the icon.
- Fix it so that &amp;gt; isn't necessary. (Somewhere &gt; is being turned
  into >)
- Make a "CallFilter" function that allows people to do:
  return CallFilter('hash2string',{format => '<a href="%{url}">%{name}</a>'},@_);
- Make it so that DU doesn't stop when there is no data.
- Make a web program that takes a url/handler and an email address, and then
  tells the person when the URL changes.
- Make a syntax printer for handlers.
- Do file locking so that running as a cron job, and running manually don't
  collide.
- Move time and date style info to the Output function.
- Disallow -a flag with -n flag to prevent abuse.
- Check into the prospect of firing off multiple threads to download handlers
  and HTML.
- Right now DU is playing the role of data grabber and HTML preprocessor. Is
  there a better way to structure it so that people can easily use m4? Or is
  it good enough as is?
- Make selectkeys so it always returns a hash. Check other filters for
  regularity.
- Verify that "make NewsClipper_Cleanup" works better on Windows.
- explain Acquisition, Filter, Output dirs in documentation
- output whole <!--newsclipper...--> command in debug info
- "imput" breaks script, as does old newsclipper syntax. Need to die
  gracefully here.
- Make a handler validation service to help people write good handlers.
- Need to fix handler download to prevent installation of handlers that don't
  work with older versions of News Clipper.
- Support for servers that require htaccess authentication
- SOCKS support
- Fix the database to be able to search multiple fields for the same string.
  (Especially created & modified together.) Have a report of the newest X
  handlers on the handlers page.
- Fix the #! for Win32 installations.
- Figure out how to do chmod in the Makefile.PL so that everything isn't
  read-only. (Works for Unix, but not Windows.)
- Make the install routine okay when overwriting old installations.
- Make a RPM distribution.
- Tooltips: Some website present information-less headlines, and a blurb
  explaining the headline. It would be nice if the blurb popped up when the
  user's mouse moved over the link.
- Real manipulation of the output HTML syntax tree: Right now I just output a
  hunk of HTML when a tag is seen. But for more advanced stuff, like
  javascript in the header, I should really be manipulating the abstract
  syntax tree for the output HTML.
- Cache the web pages locally for faster access and off-line reading. This
  pushes News Clipper into the realm of agents, which I'm not sure I want to
  venture into... (However, it would not be difficult to adapt cacheimages to
  make a cachelinks filter.)
- Implement support for last modified checks based on web server information.
- Write a handler to interface DU to local information stored in databases.
- Allow alternative means of using News Clipper for people who don't have
  shell and Perl access. Rob Saunders suggested FTP support so DU would FTP
  its output to the user's "free" account. Along those same lines, one could
  build a customizable information website using DU as the back-end.
  s based on web server information.
- Write a handler to interface DU to local information stored in databases.
- Allow alternative means of using News Clipper for people who don't have
  shell and Perl access. Rob Saunders suggested FTP support so DU would FTP
  its output to the user's "free" account. Along those same lines, one could
  build a customizable information website using DU as the back-end.
